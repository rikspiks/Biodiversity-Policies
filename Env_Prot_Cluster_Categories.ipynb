{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bc1ac14",
   "metadata": {},
   "source": [
    "# Text Clustering using pre-defiend Cluster Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef08b03",
   "metadata": {},
   "source": [
    "This script uses Kmeans clustering with predefined clusters to categorize \"Instrument\", \"Goal\", and \"Affected Group\" of environmental protection policies, based on word appearance in provided policy abstracts.\n",
    "\n",
    "Structure of the Script: \n",
    "\n",
    "1. Loading Data, preprocessing and Vectorizing the Texts\n",
    "\n",
    "2. Instrument Clusters: Defining Words, Vectorizing, Defining Clusters, Saving\n",
    "\n",
    "3. Goal Clusters: Defining Words, Vectorizing, Defining Clusters, Saving\n",
    "\n",
    "4. Affected Group Clusters: Defining Words, Vectorizing, Defining Clusters, Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82bc711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Load data from Excel\n",
    "df = pd.read_excel('env_protection.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ffdfce",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "1. Abstracts einlesen\n",
    "2. Tokenization\n",
    "3. Stopwords\n",
    "4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc4ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_list = df['Abstract_en'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3840783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import packages for Tokenization, Stopword list and Lemmatization, \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# Tokenization and Normalization\n",
    "tokenized = [word_tokenize(doc.lower()) for doc in abstracts_list]\n",
    "#Download Stopword list\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "#Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#exclude additional words\n",
    "custom_stop_words = {'shall', 'act', \"law\", \"provides\", \"provision\", \"i\", \"vi\", ',', '.', ')', 'art', '(', \"'\", \":\", \"1\"}  # Add your custom stopwords here\n",
    "stop_words.update(custom_stop_words)\n",
    "stop_words=list(stop_words)\n",
    "\n",
    "filtered = [[token for token in tokens if token not in stop_words] for tokens in tokenized]\n",
    "#Convert each sublist into a string, we need this for lemmatization\n",
    "filtered = [' '.join(sublist) for sublist in filtered]\n",
    "#Lemmanization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(token) for token in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b06e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Package for Feature Extraction using the Tf IDF Method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Feature Extraction\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_df=0.7, min_df=2)\n",
    "X = vectorizer.fit_transform(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904d9bf",
   "metadata": {},
   "source": [
    "### Instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0301485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1479)\n"
     ]
    }
   ],
   "source": [
    "# List of specific words for initial centroids, with multiple words per cluster\n",
    "instrument_seeds = [\n",
    "    ['planning', \"planners\", \"planner\" ],  \n",
    "    [\"area\", \"protected\", \"areas\"],\n",
    "    [\"regulating\", \"regulatory\", \"norms\", \"norm\", \"criteria\", \"council\", \"framework\"],\n",
    "    [\"education\", \"educational\", \"information\", \"research\", \"monitoring\", \"data\", \"Impact Assessment\"],\n",
    "    ['tax', 'subsidies', 'financing', \"finance\", \"fund\", \"subsidie\"]  \n",
    "]\n",
    "\n",
    "# Find the indices of the initial words in the vocabulary and compute the mean of TF-IDF vectors for each group\n",
    "initial_centroids = []\n",
    "for group in instrument_seeds:\n",
    "    indices = [vectorizer.vocabulary_.get(word, -1) for word in group]  # Get index or -1 if not found\n",
    "    indices = [idx for idx in indices if idx != -1]  # Remove -1 (words not found in vocabulary)\n",
    "    \n",
    "    if not indices:\n",
    "        raise ValueError(f\"None of the words in the group {group} were found in the vocabulary.\")\n",
    "    \n",
    "    # Compute mean of TF-IDF vectors for words in the group\n",
    "    centroid_vector = np.mean(X[:, indices].toarray(), axis=0)  \n",
    "    \n",
    "    # Ensure centroid_vector has right shape by padding with zeros if necessary\n",
    "    if len(centroid_vector) < X.shape[1]:\n",
    "        padded_centroid = np.zeros(X.shape[1])\n",
    "        padded_centroid[indices] = centroid_vector\n",
    "        centroid_vector = padded_centroid\n",
    "    \n",
    "    initial_centroids.append(centroid_vector)\n",
    "\n",
    "# Convert the list of centroids into a numpy array\n",
    "initial_centroids = np.array(initial_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728b1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster-Labels: [1 0 3 0 4 4 4 3 0 4 3 1 3 1 1 4 3 4 2 4 1 2 0 1 1 0 3 3 2 0 1 2 0 0 3 1 3\n",
      " 4 1 2 4 1 3 3 2 3 3 3 3 2 2 3 2 1 4 3 3 2 1 1 1 1 1 1 1 1 1 1 1 1 0 3 1 2\n",
      " 4 2 1 2 4 4 3 1 3 3 3 3 3 3 1 3 3 3 4 0 3 2 2 2 3 1 2 0 0 3 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Kmeans clustering with initial centroids\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=len(instrument_seeds), init=initial_centroids, n_init=1)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aa79c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming and saving the Instrument Classification\n",
    "Cluster_Num = kmeans.labels_\n",
    "df['Instrument'] = Cluster_Num\n",
    "Cluster_Name = {0: \"Planning\", \n",
    "                1: \"Protected Areas\", \n",
    "                2: \"Regulatory\", \n",
    "                3: \"Education_and_Research\", \n",
    "                4: \"Financial\"}\n",
    "df['Instrument_Name'] = df['Instrument'].map(Cluster_Name)\n",
    "df.to_excel('env_protection.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8687aab",
   "metadata": {},
   "source": [
    "### Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "084e99d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1479)\n"
     ]
    }
   ],
   "source": [
    "# List of specific words for initial centroids, with multiple words per cluster\n",
    "goal_seeds = [\n",
    "    ['conservation', 'preservation', 'habitat', \"restoration\", \"protected\", \"repair\"],\n",
    "    ['management', \"rational use\"],\n",
    "    [\"development\", \"agriculture\", \"productive\" ], \n",
    "    [\"education\", \"educational\", \"information\", \"research\", \"awareness\"]\n",
    "]\n",
    "\n",
    "# Find the indices of the initial words in the vocabulary and compute the mean of TF-IDF vectors for each group\n",
    "initial_centroids = []\n",
    "for group in goal_seeds:\n",
    "    indices = [vectorizer.vocabulary_.get(word, -1) for word in group]  # Get index or -1 if not found\n",
    "    indices = [idx for idx in indices if idx != -1]  # Remove -1 (words not found in vocabulary)\n",
    "    \n",
    "    if not indices:\n",
    "        raise ValueError(f\"None of the words in the group {group} were found in the vocabulary.\")\n",
    "    \n",
    "    # Compute mean of TF-IDF vectors for words in the group\n",
    "    centroid_vector = np.mean(X[:, indices].toarray(), axis=0)  \n",
    "    \n",
    "    # Ensure centroid_vector has shape (1479,) by padding with zeros if necessary\n",
    "    if len(centroid_vector) < X.shape[1]:\n",
    "        padded_centroid = np.zeros(X.shape[1])\n",
    "        padded_centroid[indices] = centroid_vector\n",
    "        centroid_vector = padded_centroid\n",
    "    \n",
    "    initial_centroids.append(centroid_vector)\n",
    "\n",
    "# Convert the list of centroids into a numpy array\n",
    "initial_centroids = np.array(initial_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20f44c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster-Labels: [0 3 0 0 3 0 0 1 1 2 1 1 1 1 1 0 3 1 1 1 0 2 1 0 2 2 3 1 2 1 0 1 1 1 1 3 3\n",
      " 1 0 2 1 3 0 0 1 1 1 0 1 2 2 3 1 1 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 1\n",
      " 1 2 1 2 0 0 0 3 3 3 1 1 0 1 1 3 1 3 0 2 1 2 1 1 1 1 1 2 1 3 2 2 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=len(goal_seeds), init=initial_centroids, n_init=1)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf1f9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming and saving\n",
    "Cluster_Num = kmeans.labels_\n",
    "df['Goal'] = Cluster_Num\n",
    "Cluster_Name = {0: \"Conservation\", \n",
    "                1: \"Management\", \n",
    "                2: \"Development\", \n",
    "                3: \"Education_and_Research\"}\n",
    "df['Goal_Name'] = df['Goal'].map(Cluster_Name)\n",
    "df.to_excel('env_protection.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89154a42",
   "metadata": {},
   "source": [
    "### Affected Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3bcd2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1479)\n"
     ]
    }
   ],
   "source": [
    "# List of specific words for initial centroids, with multiple words per cluster\n",
    "affected_seeds = [\n",
    "    ['private', 'individuals', 'citizens', \"inhabitants\", \"community\", \"participation\", \"participatory\", \"individual\"],\n",
    "    ['indigenous'],\n",
    "    [\"authorities\", \"administration\"],\n",
    "    [\"farmers\", \"agriculture\", \"livestock\", \"agricultural\", \"lease\", \"pasture\", \"pastures\", \"fishing\" ],\n",
    "    [\"companies\", \"economy\", \"use\",\"trade\", \"leasing\", \"lease\", \"juridical\" ]\n",
    "]\n",
    "\n",
    "# Find the indices of the initial words in the vocabulary and compute the mean of TF-IDF vectors for each group\n",
    "initial_centroids = []\n",
    "for group in affected_seeds:\n",
    "    indices = [vectorizer.vocabulary_.get(word, -1) for word in group]  # Get index or -1 if not found\n",
    "    indices = [idx for idx in indices if idx != -1]  # Remove -1 (words not found in vocabulary)\n",
    "    \n",
    "    if not indices:\n",
    "        raise ValueError(f\"None of the words in the group {group} were found in the vocabulary.\")\n",
    "    \n",
    "    # Compute mean of TF-IDF vectors for words in the group\n",
    "    centroid_vector = np.mean(X[:, indices].toarray(), axis=0)  \n",
    "    \n",
    "    # Ensure centroid_vector has shape (1479,) by padding with zeros if necessary\n",
    "    if len(centroid_vector) < X.shape[1]:\n",
    "        padded_centroid = np.zeros(X.shape[1])\n",
    "        padded_centroid[indices] = centroid_vector\n",
    "        centroid_vector = padded_centroid\n",
    "    \n",
    "    initial_centroids.append(centroid_vector)\n",
    "\n",
    "# Convert the list of centroids into a numpy array\n",
    "initial_centroids = np.array(initial_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b5c1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster-Labels: [4 4 0 0 0 4 4 2 1 1 0 4 0 4 1 0 4 0 2 1 4 2 4 4 4 4 1 0 0 0 4 2 4 1 1 4 0\n",
      " 4 1 1 1 1 0 1 2 0 3 0 4 2 2 0 2 1 4 0 0 0 4 4 4 4 4 4 4 4 4 4 4 4 1 4 2 2\n",
      " 0 1 2 2 2 4 4 4 1 0 0 0 0 0 4 1 1 1 4 1 0 2 2 2 4 1 2 4 0 0 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=len(affected_seeds), init=initial_centroids, n_init=1)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d1076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naming and saving\n",
    "Cluster_Num = kmeans.labels_\n",
    "df['Affected_Group'] = Cluster_Num\n",
    "Cluster_Name = {0: \"Citizens\", \n",
    "                1: \"Indigenous\", \n",
    "                2: \"Authorities and administration\", \n",
    "                3: \"Farmers\",\n",
    "               4: \"Companies\"}\n",
    "df['Affected_Group_Name'] = df['Affected_Group'].map(Cluster_Name)\n",
    "df.to_excel('env_protection.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
